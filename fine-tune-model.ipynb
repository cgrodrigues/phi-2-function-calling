{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline, \n",
    "    logging,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"microsoft/phi-2\"\n",
    "\n",
    "new_model_name = \"phi-2-function-calling\"\n",
    "new_model_path = f\"./{new_model_name}\"\n",
    "\n",
    "train_model_name = f\"{new_model_name}-train\"\n",
    "train_model_path = f\"./{train_model_name}\"\n",
    "\n",
    "adapter_name = f\"{new_model_name}-adapter\"\n",
    "adapter_path = f\"./{new_model_name}-adapter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "!huggingface-cli login --token $hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = np.load('./data/messages.npy', allow_pickle=True)\n",
    "\n",
    "data = {\n",
    "    \"text\": messages #[:100]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training (90%) and test set (10%)  \n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load base model(Phi-2)\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path ,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "# new tokens\n",
    "new_tokens =[\"<|im_start|>\", \"<|pad|>\"]\n",
    "# add the tokens to the tokenizer vocabulary\n",
    "tokenizer.add_tokens(list(new_tokens))\n",
    "tokenizer.pad_token = \"<|pad|>\"\n",
    "tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\n",
    "\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "\n",
    "def tokenize(input):\n",
    "    max_length = 1024 \n",
    "    input_ids, attention_mask, labels = [], [], [] \n",
    "    message = [input['text']['system'],\n",
    "               input['text']['user'],\n",
    "               input['text']['assistant']]\n",
    "   \n",
    "    for i, msg in enumerate(message):\n",
    "        msg_tokenized = tokenizer(  \n",
    "          msg,   \n",
    "          truncation=False,   \n",
    "          add_special_tokens=False)  \n",
    "  \n",
    "        # Copy tokens and attention mask without changes  \n",
    "        input_ids += msg_tokenized[\"input_ids\"]  \n",
    "        attention_mask += msg_tokenized[\"attention_mask\"]\n",
    "        \n",
    "        # Adapt labels for loss calculation: if system or user ->IGNORE_INDEX, \n",
    "        # if assistant->input_ids  (calculate loss only for assistant messages)      \n",
    "        if i == 2:\n",
    "            labels += msg_tokenized[\"input_ids\"]  \n",
    "        else:\n",
    "            labels += [IGNORE_INDEX]*len(msg_tokenized[\"input_ids\"]) \n",
    "    \n",
    "    # truncate to max. length  \n",
    "    return {  \n",
    "        \"input_ids\": input_ids[:max_length],   \n",
    "        \"attention_mask\": attention_mask[:max_length],  \n",
    "        \"labels\": labels[:max_length],  \n",
    "    }  \n",
    "\n",
    "        \n",
    "dataset_tokenized = dataset.map(tokenize,   \n",
    "            batched = False,  \n",
    "            num_proc = os.cpu_count(),    # multithreaded  \n",
    "            remove_columns = dataset[\"train\"].column_names  # Remove original columns, no longer needed  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense',\n",
    "    ],\n",
    "    modules_to_save = [\"lm_head\", \"embed_tokens\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    \n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate(elements):\n",
    "    tokens=[e[\"input_ids\"] for e in elements]\n",
    "    tokens_maxlen=max([len(t) for t in tokens])\n",
    "\n",
    "    for i,sample in enumerate(elements):\n",
    "        input_ids=sample[\"input_ids\"]\n",
    "        labels=sample[\"labels\"]\n",
    "        attention_mask=sample[\"attention_mask\"]\n",
    "\n",
    "        pad_len=tokens_maxlen-len(input_ids)\n",
    "\n",
    "        input_ids.extend( pad_len * [tokenizer.pad_token_id] )   \n",
    "        labels.extend( pad_len * [IGNORE_INDEX] )    \n",
    "        attention_mask.extend( pad_len * [0] ) \n",
    "\n",
    "    batch={\n",
    "        \"input_ids\": torch.tensor( [e[\"input_ids\"] for e in elements] ),\n",
    "        \"labels\": torch.tensor( [e[\"labels\"] for e in elements] ),\n",
    "        \"attention_mask\": torch.tensor( [e[\"attention_mask\"] for e in elements] ),\n",
    "    }\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size=2\n",
    "eval_batch_size=16\n",
    "ga_steps=16  # gradient acc. steps  \n",
    "steps_per_epoch=len(dataset_tokenized[\"train\"])//(train_batch_size*ga_steps)  \n",
    "epochs=1\n",
    "lr=2e-4 #0.00002  \n",
    "\n",
    "training_arguments =  SFTConfig(\n",
    "    output_dir=train_model_path,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    optim=\"paged_adamw_32bit\", \n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50, \n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=lr,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    disable_tqdm=False,\n",
    "    max_seq_length= 2048,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing= False,\n",
    "    # report_to=\"tensorboard\",  \n",
    "    report_to=\"wandb\",\n",
    "    run_name=train_model_name,\n",
    "\n",
    "    eval_steps=steps_per_epoch//5,      # eval 5 times per epoch  \n",
    "    save_steps=steps_per_epoch,         # save once per epoch  \n",
    "    lr_scheduler_type=\"constant\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_tokenized[\"train\"],  \n",
    "    eval_dataset=dataset_tokenized[\"test\"],\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    data_collator=collate,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log to a wandb project\n",
    "import wandb\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"kensei-phi2\",\n",
    "    name=\"testrun-4.8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, the trainer will have logged information including evaluation metrics\n",
    "metrics = trainer.state.log_history\n",
    "\n",
    "# Filter out the evaluation metrics\n",
    "eval_metrics = [log for log in metrics if 'eval_loss' in log]\n",
    "\n",
    "# Print evaluation metrics\n",
    "for i, metric in enumerate(eval_metrics):\n",
    "    print(f\"Evaluation {i+1}:\")\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluation after training\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "# Print out the evaluation metrics\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the adapter and push to hub\n",
    "trainer.model.save_pretrained(adapter_path, token=True)\n",
    "trainer.model.push_to_hub(f\"DataKensei/{adapter_name}\", token=True, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "# Merge the model with adapter\n",
    "new_model = trainer.model.merge_and_unload()\n",
    "\n",
    "# Save merged model and push to hub\n",
    "\n",
    "# Model\n",
    "new_model.save_pretrained(new_model_path, token=True)\n",
    "# Tokenizer\n",
    "tokenizer.save_pretrained(new_model_path)\n",
    "# Generation configuration\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=100, \n",
    "    temperature=0.7,\n",
    "    top_p=0.1,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.18,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "generation_config.save_pretrained(new_model_path)\n",
    "\n",
    "# Upload\n",
    "new_model.push_to_hub(f\"DataKensei/{new_model_name}\", token=True, safe_serialization=True)\n",
    "tokenizer.push_to_hub(f\"DataKensei/{new_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = '''<|im_start|system\n",
    "You are a helpful assistant with access to the following functions. Use these functions when they are relevant to assist with a user's request\n",
    "[\n",
    "\t{\n",
    "\t\t\"name\": \"calculate_retirement_savings\",\n",
    "\t\t\"description\": \"Project the savings at retirement based on current contributions.\",\n",
    "\t\t\"parameters\": {\n",
    "\t\t\t\"type\": \"object\",\n",
    "\t\t\t\"properties\": {\n",
    "\t\t\t\t\"current_age\": {\n",
    "\t\t\t\t\t\"type\": \"integer\",\n",
    "\t\t\t\t\t\"description\": \"The current age of the individual.\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"retirement_age\": {\n",
    "\t\t\t\t\t\"type\": \"integer\",\n",
    "\t\t\t\t\t\"description\": \"The desired retirement age.\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"current_savings\": {\n",
    "\t\t\t\t\t\"type\": \"number\",\n",
    "\t\t\t\t\t\"description\": \"The current amount of savings.\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"monthly_contribution\": {\n",
    "\t\t\t\t\t\"type\": \"number\",\n",
    "\t\t\t\t\t\"description\": \"The monthly contribution towards retirement savings.\"\n",
    "\t\t\t\t}\n",
    "\t\t\t},\n",
    "\t\t\t\"required\": [\"current_age\", \"retirement_age\", \"current_savings\", \"monthly_contribution\"]\n",
    "\t\t}\n",
    "\t}\n",
    "]<|im_end|>\n",
    "<|im_start|user\n",
    "I am currently 40 years old and plan to retire at 65. I have no savings at the moment, but I intend to save $500 every month. Could you project the savings at retirement based on current contributions?<|im_end|>\n",
    "'''\n",
    "pipe = pipeline(task=\"text-generation\", model=new_model, tokenizer=tokenizer, max_length=500)\n",
    "result = pipe(prompt)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = '''<|im_start|>system\n",
    "You are a helpful assistant with access to the following functions. Use these functions when they are relevant to assist with a user's request\n",
    "[{\n",
    "    \"name\": \"schedule_meeting\",\n",
    "            \"description\": \"Schedule a meeting on the user's calendar.\",\n",
    "    \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"date\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The date of the meeting.\"\n",
    "            },\n",
    "            \"time\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The time of the meeting.\"\n",
    "            },\n",
    "            \"participants\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"The email addresses of the participants.\"\n",
    "            },\n",
    "            \"duration\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The duration of the meeting.\"\n",
    "            },\n",
    "        },\n",
    "        \"required\": ['date', 'time', 'participants']\n",
    "    }\n",
    "}]<|im_end|>\n",
    "<|im_start|>user\n",
    "Can you arrange a meeting on 2024/10/01 at 13:00 with ['Erin Mendez', 'Craig Shields', 'Jennifer Mclaughlin', 'Jason Curry', 'Heidi Coleman', 'Patricia Booth', 'Laura Willis', 'Sarah Cruz', 'Matthew Hale', 'Kirk Reynolds']?<|im_end|>\n",
    "'''\n",
    "pipe = pipeline(task=\"text-generation\", model=new_model, tokenizer=tokenizer, max_length=500)\n",
    "result = pipe(prompt)\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

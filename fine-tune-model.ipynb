{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Phi-2 Model for Function Calling on a Synthetic Dataset\n",
    "\n",
    "In this notebook, we explore the process of fine-tuning a Phi-2 language model for function calling tasks using a synthetic dataset. The Phi-2 model is a small language model originally trained on synthetic datasets designed to enhance common-sense reasoning and general knowledge. In this notebook, we adapt the model to handle specific function calling scenarios, which is particularly relevant in applications where a model needs to interpret user requests and accurately execute predefined functions based on the input.\n",
    "\n",
    "The primary objective of this notebook is to provide a comprehensive guide to fine-tuning a language model for specialized tasks, showcasing the steps required to adapt a general-purpose model to handle specific, structured interactions in a conversational context. By the end of this notebook, you will have a fully fine-tuned Phi-2 model capable of handling function calling tasks on a synthetic dataset, along with an understanding of the key concepts and techniques involved in the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Includes all the necessary import statements required to execute the subsequent code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import wandb\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import setup_chat_format\n",
    "from peft import LoraConfig, get_peft_model, cast_mixed_precision_params, prepare_model_for_kbit_training\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import math\n",
    "from dotenv import load_dotenv\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/cgrodrigues/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "!huggingface-cli login --token $hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Generated Dataset\n",
    "\n",
    "In this section, we load a dataset that has been pre-generated and stored in a NumPy array file. The dataset consists of chat messages that include system, user, and assistant roles. The code performs the following steps:\n",
    "1. Loading the dataset from the file.\n",
    "2. Converting the data to a dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = np.load('./data/messages.npy', allow_pickle=True)\n",
    "\n",
    "# messages = [f\"{m['system']}{m['user']}{m['assistant']}\" for m in messages]\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"messages_templated\": messages \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Base Model and the Tokenizer \n",
    "\n",
    "In this section, we load a pre-trained language model and its corresponding tokenizer, along with configuring the model for efficient inference using quantization techniques. The steps involved are as follows:\n",
    "\n",
    "1. Define the model_id of the pre-trained model to use. In this case, we are loading the \"microsoft/phi-2\" model, which is a pre-trained language model from Microsoft.\n",
    "2. Load the tokenizer. This tokenizer is essential for converting text into a format that the model can process (i.e., token IDs) and for converting the model's output back into human-readable text.\n",
    "3. Use a BitsAndBytesConfig object to configure the model's quantization settings. This configuration reduce the model's memory footprint and improve inference speed by:\n",
    "    * Loading the model in 4-bit precision.\n",
    "    * Setting the quantization data type to \"nf4\" (4-bit NormalFloat).\n",
    "    * The computation data type as torch.bfloat16.\n",
    "    * And not to use double quantization.\n",
    "4. Load the Pre-trained Model.\n",
    "\n",
    "\n",
    "https://huggingface.co/blog/4bit-transformers-bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce06102482f4f8e8c0122f5b55e7bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_path = \"microsoft/phi-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "  )\n",
    "\n",
    "model.config.use_cache = False # disable the use of the cache during the \n",
    "                               # generation process.\n",
    "model.config.pretraining_tp = 1 # disables tensor parallelism, the model is\n",
    "                                # not split across multiple devices and runs\n",
    "                                # on a single device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Tokenizer Chat Format\n",
    "\n",
    "In this section, we configure the model and tokenizer to use a specific chat format, referred to as the 'chatml' format. This setup is likely necessary to ensure that the inputs and outputs between the model, tokenizer, and chat interface are compatible and properly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the chat format with default 'chatml' format\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a nice assistant.<|im_end|>\\n<|im_start|>user\\nHello, there!<|im_end|>\\n<|im_start|>assistant\\nHi!<|im_end|>\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template([{\n",
    "                                \"role\": \"system\", \n",
    "                                \"content\": \"You are a nice assistant.\"\n",
    "                               },\n",
    "                               {\n",
    "                                \"role\": \"user\", \n",
    "                                \"content\": \"Hello, there!\"\n",
    "                               }, \n",
    "                               {\n",
    "                                \"role\": \"assistant\", \n",
    "                                \"content\": \"Hi!\"\n",
    "                               }], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiForCausalLM(\n",
      "  (model): PhiModel(\n",
      "    (embed_tokens): Embedding(50297, 2560)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x PhiDecoderLayer(\n",
      "        (self_attn): PhiSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (rotary_emb): PhiRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): PhiMLP(\n",
      "          (activation_fn): NewGELUActivation()\n",
      "          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
      "          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=50297, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset for Training and Testing\n",
    "\n",
    "In this section, the dataset is prepared for training and testing by shuffling, selecting a subset of samples, and splitting the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages_templated'],\n",
       "        num_rows: 1260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages_templated'],\n",
       "        num_rows: 140\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Number of Samples:\n",
    "NUM_SAMPLES = 1400 #128_000\n",
    "\n",
    "# Shuffle the Dataset\n",
    "dataset = dataset.shuffle(seed=42).select(range(NUM_SAMPLES))\n",
    "\n",
    "# Split the Dataset into Training and Test Sets\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assistant': '<|im_start|>assistant\\n<functioncall> {\"name\": \"search_internet\", \"arguments\": \"{\\'query\\': \\'Far modern myself.\\', \\'language\\': \\'industry\\', \\'filter_date\\': \\'account\\'}\"} <|im_end|><|endoftext|>', 'system': '<|im_start|>system\\nYou are a helpful assistant with access to the following functions. Use these functions when they are relevant to assist with a user\\'s request\\n[{\\n    \"name\": \"search_internet\",\\n    \"description\": \"Perform an internet search based on the user\\'s query.\",\\n    \"parameters\": {\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"query\": {\\n                \"type\": \"string\",\\n                \"description\": \"The search query.\"\\n            },\\n            \"language\": {\\n                \"type\": \"string\",\\n                \"description\": \"The language of the search results.\"\\n            },\\n            \"filter_date\": {\\n                \"type\": \"string\",\\n                \"description\": \"A date filter for the search results.\"\\n            },\\n        },\\n        \"required\": [\\'query\\']\\n    }\\n}]<|im_end|>\\n', 'user': \"<|im_start|>user\\nI need to search for 'Far modern myself.' online.<|im_end|>\\n\"}\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "{'assistant': '<|im_start|>assistant\\n<functioncall> {\"name\": \"calculate_loan_payment\", \"arguments\": \"{\\'loan_amount\\': 323269, \\'interest_rate\\': 51.0971502411072, \\'term_years\\': 29}\"} <|im_end|><|endoftext|>', 'system': '<|im_start|>system\\nYou are a helpful assistant with access to the following functions. Use these functions when they are relevant to assist with a user\\'s request\\n[{\\n    \"name\": \"calculate_loan_payment\",\\n    \"description\": \"Calculate the monthly payment for a loan.\",\\n    \"parameters\": {\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"loan_amount\": {\\n                \"type\": \"number\",\\n                \"description\": \"The total amount of the loan.\"\\n            },\\n            \"interest_rate\": {\\n                \"type\": \"number\",\\n                \"description\": \"The annual interest rate of the loan.\"\\n            },\\n            \"term_years\": {\\n                \"type\": \"integer\",\\n                \"description\": \"The term of the loan in years.\"\\n            },\\n        },\\n        \"required\": [\\'loan_amount\\', \\'interest_rate\\', \\'term_years\\']\\n    }\\n}]<|im_end|>\\n', 'user': '<|im_start|>user\\nCan you calculate the monthly payment for a loan of 323269 with an interest rate of 51.0971502411072% over 29 years?<|im_end|>\\n'}\n",
      "\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some examples\n",
    "for i in range(2):\n",
    "  print(dataset[\"train\"][\"messages_templated\"][i])\n",
    "  print(\"\\n-------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization the Dataset and Collation Function \n",
    "\n",
    "Hear the dataset is tokenized to all the message have a maximum size of 1024 tokens, also, due the model should not learn from teh user and system messages,  the messages from user and system are karket with label IGNORE to no be used in the loss calculation during the training. In the second part is create a collation function to use to process the bactches during the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316cc1c9b38c48639216087847f7764b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/1260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac70d04cf13484bbc56d67d53559098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IGNORE_INDEX = -100\n",
    "\n",
    "def tokenize(input):\n",
    "    max_length = 1024 \n",
    "    input_ids, attention_mask, labels = [], [], [] \n",
    "    message = [input['messages_templated']['system'],\n",
    "               input['messages_templated']['user'],\n",
    "               input['messages_templated']['assistant']]\n",
    "   \n",
    "    for i, msg in enumerate(message):\n",
    "        msg_tokenized = tokenizer(  \n",
    "          msg,   \n",
    "          truncation=False,   \n",
    "          add_special_tokens=False)  \n",
    "  \n",
    "        # Copy tokens and attention mask without changes  \n",
    "        input_ids += msg_tokenized[\"input_ids\"]  \n",
    "        attention_mask += msg_tokenized[\"attention_mask\"]\n",
    "        \n",
    "        # Adapt labels for loss calculation: if system or user ->IGNORE_INDEX, \n",
    "        # if assistant->input_ids  (calculate loss only for assistant messages)      \n",
    "        if i == 2:\n",
    "            labels += msg_tokenized[\"input_ids\"]  \n",
    "        else:\n",
    "            labels += [IGNORE_INDEX]*len(msg_tokenized[\"input_ids\"]) \n",
    "    \n",
    "    # truncate to max. length  \n",
    "    return {  \n",
    "        \"input_ids\": input_ids[:max_length],   \n",
    "        \"attention_mask\": attention_mask[:max_length],  \n",
    "        \"labels\": labels[:max_length],  \n",
    "    }  \n",
    "\n",
    "        \n",
    "dataset_tokenized = dataset.map(tokenize,   \n",
    "            batched = False,  \n",
    "            num_proc = os.cpu_count(),    # multithreaded  \n",
    "            remove_columns = dataset[\"train\"].column_names  # Remove original columns, no longer needed  \n",
    ")\n",
    "\n",
    "def collate(elements):\n",
    "    tokens=[e[\"input_ids\"] for e in elements]\n",
    "    tokens_maxlen=max([len(t) for t in tokens])\n",
    "\n",
    "    for i,sample in enumerate(elements):\n",
    "        input_ids=sample[\"input_ids\"]\n",
    "        labels=sample[\"labels\"]\n",
    "        attention_mask=sample[\"attention_mask\"]\n",
    "\n",
    "        pad_len=tokens_maxlen-len(input_ids)\n",
    "\n",
    "        input_ids.extend( pad_len * [tokenizer.pad_token_id] )   \n",
    "        labels.extend( pad_len * [IGNORE_INDEX] )    \n",
    "        attention_mask.extend( pad_len * [0] ) \n",
    "\n",
    "    batch={\n",
    "        \"input_ids\": torch.tensor( [e[\"input_ids\"] for e in elements] ),\n",
    "        \"labels\": torch.tensor( [e[\"labels\"] for e in elements] ),\n",
    "        \"attention_mask\": torch.tensor( [e[\"attention_mask\"] for e in elements] ),\n",
    "    }\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 140\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Apply LoRA (Low-Rank Adaptation) for Model Fine-Tuning\n",
    "\n",
    "In this section, we configure and apply LoRA (Low-Rank Adaptation) to the model, a technique used to efficiently fine-tune large language models by adapting specific parts of the model. The code involves setting up the LoRA configuration, applying it to the model and adjusting the precision of model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 304,756,857 || all params: 3,079,816,434 || trainable%: 9.8953\n"
     ]
    }
   ],
   "source": [
    "# Prepares the model for training using low-bit \n",
    "# precision (8-bit or 4-bit precision)\n",
    "model = prepare_model_for_kbit_training(model, \n",
    "                                        use_gradient_checkpointing=True)\n",
    "\n",
    "# LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=32, # This parameter controls the rank of the low-rank adaptation \n",
    "          # matrices. A lower rank reduces the number of parameters, \n",
    "          # making the adaptation more efficient.\n",
    "\n",
    "    lora_alpha=32, # This scaling factor balances the impact of the \n",
    "                   # low-rank adaptation. It helps control how much \n",
    "                   # the adaptation affects the original model parameters.\n",
    "                   \n",
    "    # Target all linear layers\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense',\n",
    "        'fc1',\n",
    "        'fc2',\n",
    "    ], # These are the names of the linear layers in the model that will be \n",
    "       # adapted using LoRA. By targeting these modules, LoRA modifies only \n",
    "       # these parts of the model, making fine-tuning more efficient.\n",
    "\n",
    "    modules_to_save = [\"lm_head\", \"embed_tokens\"],\n",
    "    lora_dropout=0.05, # A small dropout rate is applied during training to \n",
    "                      # prevent overfitting. This means 5% of the neurons \n",
    "                      # will be randomly dropped during training.\n",
    "\n",
    "    bias=\"none\", # No additional bias is added in the adaptation layers.\n",
    "    task_type=\"CAUSAL_LM\", # Specifies that the task is causal language \n",
    "                           # modeling, indicating that the model predicts \n",
    "                           # the next word in a sequence.\n",
    "    \n",
    ")\n",
    "\n",
    "# Apply LoRA to the Model:\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Cast Mixed Precision Parameters:\n",
    "# cast_mixed_precision_params(model, dtype=torch.float16)\n",
    "# The model's parameters are cast to torch.float16. This reduces memory \n",
    "# usage and speeds up computation during training\n",
    "\n",
    "# Print Trainable Parameters:\n",
    "model.print_trainable_parameters()\n",
    "# The output of this line said the number of trainable parameters \n",
    "# in the model after applying LoRA.\n",
    "\n",
    "model.config.use_cache = False # disable the use of the cache \n",
    "                               # during the generation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Training Configuration and Initialize the Fine-Tuning Trainer\n",
    "\n",
    "Here is defined various hyperparameters and configurations necessary for fine-tuning the model. The configuration is encapsulated in a SFTConfig object. Then initialize the SFTTrainer the handles the training loop, evaluation, and model saving according to the parameters defined in the SFTConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Steps: 50\n",
      "Save Steps: 100\n"
     ]
    }
   ],
   "source": [
    "# Define Hyperparameters:\n",
    "max_seq_length = 2048 # Specifies the maximum sequence length for the model, \n",
    "                      # meaning each input sequence can be up to 2048 tokens long.\n",
    "batch_size = 2 # Sets the number of samples processed per batch. A batch size \n",
    "               # of 1 chosen due to memory constraints.\n",
    "gradient_accum_steps = 1 # Determines how many batches to process before performing \n",
    "                         # a gradient update. Since it's set to 1, gradients are \n",
    "                         # updated after every batch.\n",
    "epochs = 1 # Indicates that the model will be trained for one complete pass \n",
    "           # through the dataset.\n",
    "eval_steps = 50 # The model's performance will be evaluated every 50 steps.\n",
    "save_steps = eval_steps * 2 # The model will be saved every 100 steps \n",
    "                            # (twice as often as evaluation).\n",
    "logging_steps = 50 # Logging information will be output every 50 steps, \n",
    "                   # aligning with evaluation intervals.\n",
    "lr = 2e-5   # Sets the learning rate for training. This controls how much to adjust \n",
    "            # the model's parameters with respect to the loss gradient during each update.\n",
    "\n",
    "print(\"Eval Steps:\", eval_steps)\n",
    "print(\"Save Steps:\", save_steps)\n",
    "\n",
    "# Set Model and Adapter Path and Names\n",
    "\n",
    "new_model_name = \"phi-2-function-calling\"\n",
    "new_model_path = f\"./{new_model_name}\"\n",
    "\n",
    "train_model_name = f\"{new_model_name}-train\"\n",
    "train_model_path = f\"./{train_model_name}\"\n",
    "\n",
    "adapter_name = f\"{new_model_name}-adapter\"\n",
    "adapter_path = f\"./{new_model_name}-adapter\"\n",
    "\n",
    "# Configure Training with SFTConfig:\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=train_model_path, # Where the trained model will be saved.\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accum_steps,\n",
    "    \n",
    "    optim=\"paged_adamw_32bit\", \n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=logging_steps,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=lr, \n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    disable_tqdm=False,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_text_field=\"messages_templated\", # Which field in the dataset contains the text data to be used.\n",
    "    packing=False, \n",
    "    report_to=\"wandb\", # Training metrics will be reported to Weights & Biases\n",
    "    run_name=train_model_name,\n",
    "    eval_steps=eval_steps,\n",
    "    save_steps=save_steps,\n",
    "    lr_scheduler_type=\"constant\", \n",
    "    eval_strategy=\"steps\",\n",
    "  )\n",
    "\n",
    "\n",
    "# Initialize the SFTTrainer:\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_tokenized['train'],\n",
    "    eval_dataset=dataset_tokenized['test'],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    "    data_collator=collate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weights & Biases (W&B) Tracking for the Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcarlos-garces-rodrigues\u001b[0m (\u001b[33mdatakensei\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cgrodrigues/projects/phi-2-function-calling/test/wandb/run-20240823_175150-zzxroai0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/datakensei/kensei-phi2/runs/zzxroai0' target=\"_blank\">testrun-4.8.3</a></strong> to <a href='https://wandb.ai/datakensei/kensei-phi2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/datakensei/kensei-phi2' target=\"_blank\">https://wandb.ai/datakensei/kensei-phi2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/datakensei/kensei-phi2/runs/zzxroai0' target=\"_blank\">https://wandb.ai/datakensei/kensei-phi2/runs/zzxroai0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"kensei-phi2\",\n",
    "    name=\"testrun-4.8.3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model Before Training\n",
    "\n",
    "The model is evaluated on the test dataset, and the perplexity is calculated. Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_results = trainer.evaluate()\n",
    "# print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "# eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Model Training Process\n",
    "\n",
    "This step involves feeding the model with the training data, adjusting its parameters to minimize the loss, and iterating through the dataset according to the prevoius configuration settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgrodrigues/projects/phi-2-function-calling/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/cgrodrigues/projects/phi-2-function-calling/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 4:14:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.470500</td>\n",
       "      <td>0.759203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.562400</td>\n",
       "      <td>0.378981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.360300</td>\n",
       "      <td>0.268035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.203000</td>\n",
       "      <td>0.234188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.294700</td>\n",
       "      <td>0.223158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.345600</td>\n",
       "      <td>0.219122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.261800</td>\n",
       "      <td>0.212389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.210371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>0.210798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.218700</td>\n",
       "      <td>0.210504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.289800</td>\n",
       "      <td>0.208540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.304200</td>\n",
       "      <td>0.207028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgrodrigues/projects/phi-2-function-calling/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/cgrodrigues/projects/phi-2-function-calling/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=630, training_loss=0.3955709184919085, metrics={'train_runtime': 15336.2616, 'train_samples_per_second': 0.082, 'train_steps_per_second': 0.041, 'total_flos': 6471275893612128.0, 'train_loss': 0.3955709184919085, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# eval_results_after = trainer.evaluate()\n",
    "# print(f\">>> Perplexity: {math.exp(eval_results_after['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Chat Interaction with the New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/cgrodrigues/projects/phi-2-function-calling/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/cgrodrigues/projects/phi-2-function-calling/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "tensor([[50295, 10057,   198,  1639,   389,   257,  7613,  8796,   351,  1895,\n",
      "           284,   262,  1708,  5499,    13,  5765,   777,  5499,   618,   484,\n",
      "           389,  5981,   284,  3342,   351,   257,  2836,   338,  2581,   198,\n",
      "            58,   198,   197,    90,   198, 50294,     1,  3672,  1298,   366,\n",
      "          9948,  3129,   378,    62,  1186, 24615,    62, 39308,   654,  1600,\n",
      "           198, 50294,     1, 11213,  1298,   366, 16775,   262, 10653,   379,\n",
      "         10737,  1912,   319,  1459,  9284, 33283,   198, 50294,     1, 17143,\n",
      "          7307,  1298,  1391,   198, 50293,     1,  4906,  1298,   366, 15252,\n",
      "          1600,   198, 50293,     1, 48310,  1298,  1391,   198, 50292,     1,\n",
      "         14421,    62,   496,  1298,  1391,   198, 50291,     1,  4906,  1298,\n",
      "           366, 41433,  1600,   198, 50291,     1, 11213,  1298,   366,   464,\n",
      "          1459,  2479,   286,   262,  1981,   526,   198, 50292,  5512,   198,\n",
      "         50292,     1,  1186, 24615,    62,   496,  1298,  1391,   198, 50290,\n",
      "          2981,  1298,   366, 41433,  1600,   198, 50291,     1, 11213,  1298,\n",
      "           366,   464, 10348, 10737,  2479,   526,   198, 50292,  5512,   198,\n",
      "         50292,     1, 14421,    62, 39308,   654,  1298,  1391,   198, 50291,\n",
      "             1,  4906,  1298,   366, 17618,  1600,   198, 50291,     1, 11213,\n",
      "          1298,   366,   464,  1459,  2033,   286, 10653,   526,   198, 50292,\n",
      "          5512,   198, 50292,     1,  8424,   306,    62,  3642,  3890,  1298,\n",
      "          1391,   198, 50291,     1,  4906,  1298,   366, 17618,  1600,   198,\n",
      "         50291,     1, 11213,  1298,   366,   464,  9651, 10156,  3371, 10737,\n",
      "         10653,   526,   198, 50292,    92,   198, 50293,  5512,   198, 50293,\n",
      "             1, 35827,  1298, 14631, 14421,    62,   496,  1600,   366,  1186,\n",
      "         24615,    62,   496,  1600,   366, 14421,    62, 39308,   654,  1600,\n",
      "           366,  8424,   306,    62,  3642,  3890,  8973,   198, 50294,    92,\n",
      "           198,   197,    92,   198,    60, 50296,   198, 50295,  7220,   198,\n",
      "            40,   716,  3058,  2319,   812,  1468,   290,  1410,   284,  8058,\n",
      "           379,  6135,    13,   314,   423,   645, 10653,   379,   262,  2589,\n",
      "            11,   475,   314, 14765,   284,  3613,   720,  4059,   790,  1227,\n",
      "            13, 10347,   345,  1628,   262, 10653,   379, 10737,  1912,   319,\n",
      "          1459,  9284,    30, 50296,   198, 50295,   562, 10167,   198,    27,\n",
      "          8818, 13345,    29, 19779,  3672,  1298,   366,  9948,  3129,   378,\n",
      "            62,  1186, 24615,    62, 39308,   654,  1600,   366,   853,  2886,\n",
      "          1298, 45144,     6, 14421,    62,   496, 10354,  2319,    11,   705,\n",
      "          1186, 24615,    62,   496, 10354,  6135,    11,   705, 14421,    62,\n",
      "         39308,   654, 10354,   657,    11,   705,  8424,   306,    62,  3642,\n",
      "          3890, 10354,  5323,    92, 20662,   220, 50296]], device='cuda:0')\n",
      "===========\n",
      "<|im_start|>system\n",
      "You are a helpful assistant with access to the following functions. Use these functions when they are relevant to assist with a user's request\n",
      "[\n",
      "\t{\n",
      "\t\t\"name\": \"calculate_retirement_savings\",\n",
      "\t\t\"description\": \"Project the savings at retirement based on current contributions.\",\n",
      "\t\t\"parameters\": {\n",
      "\t\t\t\"type\": \"object\",\n",
      "\t\t\t\"properties\": {\n",
      "\t\t\t\t\"current_age\": {\n",
      "\t\t\t\t\t\"type\": \"integer\",\n",
      "\t\t\t\t\t\"description\": \"The current age of the individual.\"\n",
      "\t\t\t\t},\n",
      "\t\t\t\t\"retirement_age\": {\n",
      "\t\t\t\t\t\type\": \"integer\",\n",
      "\t\t\t\t\t\"description\": \"The desired retirement age.\"\n",
      "\t\t\t\t},\n",
      "\t\t\t\t\"current_savings\": {\n",
      "\t\t\t\t\t\"type\": \"number\",\n",
      "\t\t\t\t\t\"description\": \"The current amount of savings.\"\n",
      "\t\t\t\t},\n",
      "\t\t\t\t\"monthly_contribution\": {\n",
      "\t\t\t\t\t\"type\": \"number\",\n",
      "\t\t\t\t\t\"description\": \"The monthly contribution towards retirement savings.\"\n",
      "\t\t\t\t}\n",
      "\t\t\t},\n",
      "\t\t\t\"required\": [\"current_age\", \"retirement_age\", \"current_savings\", \"monthly_contribution\"]\n",
      "\t\t}\n",
      "\t}\n",
      "]<|im_end|>\n",
      "<|im_start|>user\n",
      "I am currently 40 years old and plan to retire at 65. I have no savings at the moment, but I intend to save $500 every month. Could you project the savings at retirement based on current contributions?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<functioncall> {\"name\": \"calculate_retirement_savings\", \"arguments\": \"{'current_age': 40,'retirement_age': 65, 'current_savings': 0,'monthly_contribution': 500}\"} <|im_end|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def chat(messages):\n",
    "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = trainer.model.generate(tokenized_chat, max_new_tokens=128) #, stopping_criteria=[\"<|im_end|>\"])\n",
    "    print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\":\"system\", \n",
    "        \"content\":\"You are a helpful assistant with access to the following functions. Use these functions when they are relevant to assist with a user's request\\n[\\n\t{\\n\t\t\\\"name\\\": \\\"calculate_retirement_savings\\\",\\n\t\t\\\"description\\\": \\\"Project the savings at retirement based on current contributions.\\\",\\n\t\t\\\"parameters\\\": {\\n\t\t\t\\\"type\\\": \\\"object\\\",\\n\t\t\t\\\"properties\\\": {\\n\t\t\t\t\\\"current_age\\\": {\\n\t\t\t\t\t\\\"type\\\": \\\"integer\\\",\\n\t\t\t\t\t\\\"description\\\": \\\"The current age of the individual.\\\"\\n\t\t\t\t},\\n\t\t\t\t\\\"retirement_age\\\": {\\n\t\t\t\t\t\\type\\\": \\\"integer\\\",\\n\t\t\t\t\t\\\"description\\\": \\\"The desired retirement age.\\\"\\n\t\t\t\t},\\n\t\t\t\t\\\"current_savings\\\": {\\n\t\t\t\t\t\\\"type\\\": \\\"number\\\",\\n\t\t\t\t\t\\\"description\\\": \\\"The current amount of savings.\\\"\\n\t\t\t\t},\\n\t\t\t\t\\\"monthly_contribution\\\": {\\n\t\t\t\t\t\\\"type\\\": \\\"number\\\",\\n\t\t\t\t\t\\\"description\\\": \\\"The monthly contribution towards retirement savings.\\\"\\n\t\t\t\t}\\n\t\t\t},\\n\t\t\t\\\"required\\\": [\\\"current_age\\\", \\\"retirement_age\\\", \\\"current_savings\\\", \\\"monthly_contribution\\\"]\\n\t\t}\\n\t}\\n]\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"I am currently 40 years old and plan to retire at 65. I have no savings at the moment, but I intend to save $500 every month. Could you project the savings at retirement based on current contributions?\"\n",
    "    }\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Model and Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the model with adapter\n",
    "new_model = trainer.model.merge_and_unload()\n",
    "\n",
    "# Save merged model and push to hub\n",
    "\n",
    "# Model\n",
    "new_model.save_pretrained(new_model_path, token=True)\n",
    "# Tokenizer\n",
    "tokenizer.save_pretrained(new_model_path)\n",
    "# Generation configuration\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=100, \n",
    "    temperature=0.7,\n",
    "    top_p=0.1,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.18,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "generation_config.save_pretrained(new_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload\n",
    "area = \"DataKensei\"\n",
    "new_model.push_to_hub(f\"{area}/{new_model_name}\", token=True, safe_serialization=True)\n",
    "tokenizer.push_to_hub(f\"{area}/{new_model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
